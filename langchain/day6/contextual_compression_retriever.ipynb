{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d976c01d",
   "metadata": {},
   "source": [
    "## 뮨맥 압축 검색기\n",
    "\n",
    "검색 시 어려움 중 하나는 데이터를 시스템에 수집할 때 어떤 질의를 처리해야 할지 미리 알 수 없다는 점이다\n",
    "\n",
    "- `ContextualCompressionRetriever`은 검색된 문서를 그대로 즉시 반환되는 대신, 주어진 질의의 맥락을 사용하여 문서를 압축함으로써 관련 정보만 반환되도록 할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9cbb38d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문서 1:\\n\\nSemantic Search\\n\\n정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\\n예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\\n연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "model = ChatOllama(\n",
    "  model = \"gemma3:latest\",\n",
    "  base_url = \"http://localhost:11434\",\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"../data/test.txt\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size = 300, \n",
    "    chunk_overlap = 0,\n",
    ")\n",
    "\n",
    "texts = loader.load_and_split(text_splitter)\n",
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82ce496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 공부를 시작하시는군요! 좋은 질문입니다. TensorFlow를 잘 다룰 줄 아신다면, 어떤 언어를 먼저 공부해야 할지, 그리고 어떤 방향으로 공부를 해야 할지에 대한 조언을 드리겠습니다.\n",
      "\n",
      "**1. 어떤 언어를 먼저 공부해야 할까요?**\n",
      "\n",
      "*   **Python:** AI 분야에서 가장 널리 사용되는 언어입니다. TensorFlow, PyTorch 등 주요 AI 프레임워크가 Python 기반으로 개발되었기 때문입니다. TensorFlow를 잘 다룰 줄 아신다면 Python을 먼저 배우는 것이 훨씬 유리합니다. Python은 배우기 쉽고, 다양한 라이브러리와 도구가 풍부하여 AI 모델 개발, 데이터 분석, 시각화 등 다양한 작업을 수행하기에 적합합니다.\n",
      "*   **C언어:** C언어는 시스템 프로그래밍, 임베디드 시스템 등 하드웨어 제어에 강점을 가지고 있습니다. 하지만 AI 모델 개발에는 Python만큼 널리 사용되지는 않습니다. TensorFlow를 사용하기 위해 C언어를 배우는 것은 일반적으로 필요하지 않습니다.\n",
      "\n",
      "**결론:** TensorFlow를 잘 다룰 줄 아신다면, **Python**을 우선적으로 공부하는 것이 좋습니다.\n",
      "\n",
      "**2. TensorFlow를 잘 다룰 줄 아는데 어떤 공부를 하는 것이 좋을까요?**\n",
      "\n",
      "TensorFlow를 잘 다룰 줄 아신다는 것은 이미 훌륭한 기반을 갖추고 계신 것을 의미합니다. 이제 TensorFlow를 더욱 깊이 이해하고 활용하기 위한 다음 단계를 준비해야 합니다. 다음은 추천하는 학습 방향입니다.\n",
      "\n",
      "*   **TensorFlow 핵심 개념 학습:**\n",
      "    *   **텐서(Tensor) 이해:** TensorFlow의 핵심 데이터 구조인 텐서에 대한 이해는 필수입니다. 텐서의 차원, 데이터 타입, 연산 등을 정확히 이해해야 합니다.\n",
      "    *   **그래프(Graph) 이해:** TensorFlow는 텐서 연산을 그래프 형태로 표현합니다. 그래프의 구조, 노드, 엣지 등에 대한 이해는 모델 개발에 매우 중요합니다.\n",
      "    *   **세션(Session) 및 실행(Execution) 이해:** 텐서플로우 그래프를 실행하기 위한 세션과 실행 과정을 이해해야 합니다.\n",
      "*   **고급 TensorFlow API 학습:**\n",
      "    *   **Keras API:** TensorFlow의 고수준 API인 Keras를 활용하여 더욱 쉽고 빠르게 모델을 구축하고 훈련할 수 있습니다.\n",
      "    *   **TensorFlow Hub:** 사전 훈련된 모델을 활용하여 모델 개발 시간을 단축하고 성능을 향상시킬 수 있습니다.\n",
      "    *   **TensorFlow Lite:** 모바일, 임베디드 시스템 등 다양한 환경에서 TensorFlow 모델을 실행하기 위한 방법을 학습합니다.\n",
      "*   **머신러닝 이론 학습:**\n",
      "    *   **지도 학습, 비지도 학습, 강화 학습:** 다양한 머신러닝 학습 방식을 이해하고, 각 방식에 적합한 모델을 선택하고 훈련하는 방법을 학습합니다.\n",
      "    *   **모델 평가 및 튜닝:** 모델의 성능을 평가하고, 하이퍼파라미터를 튜닝하여 모델의 성능을 최적화하는 방법을 학습합니다.\n",
      "    *   **과적합(Overfitting) 방지:** 모델이 훈련 데이터에만 지나치게 적합하여 새로운 데이터에 대한 성능이 떨어지는 현상을 방지하는 방법을 학습합니다.\n",
      "*   **실전 프로젝트:**\n",
      "    *   **다양한 데이터셋 활용:** MNIST, CIFAR-10 등 다양한 데이터셋을 활용하여 실제 모델을 구축하고 훈련해 보세요.\n",
      "    *   **자신만의 데이터셋 구축:** 자신만의 데이터셋을 구축하고, 이를 활용하여 모델을 개발해 보세요.\n",
      "    *   **오픈 소스 프로젝트 참여:** 오픈 소스 AI 프로젝트에 참여하여 다른 개발자들과 협업하고 실력을 향상시키세요.\n",
      "\n",
      "**추가적으로 고려할 사항:**\n",
      "\n",
      "*   **딥러닝(Deep Learning) 이론:** 딥러닝은 딥러닝 모델을 이해하고 설계하는 데 필수적인 이론입니다.\n",
      "*   **선형대수, 미적분, 확률 및 통계:** 머신러닝 및 딥러닝 이론을 이해하는 데 도움이 되는 기초 수학 지식을 학습하는 것이 좋습니다.\n",
      "\n",
      "궁금한 점이 있다면 언제든지 다시 질문해주세요. 응원합니다!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# one-shot prompting 복습\n",
    "oneshot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"너는 코딩 전문가야, question에 대답하면 돼. Let's think step by step\"),\n",
    "    (\"human\",\"ai를 공부하려는데 python이 좋을까 C언어가 좋을까?\"),\n",
    "    (\"human\",\"{question}\")\n",
    "])\n",
    "\n",
    "chain = oneshot_prompt | model | StrOutputParser()\n",
    "res = chain.invoke({\"question\":\"tensorflow를 잘 다룰 줄 아는데 어떤 공부를 하는게 좋을까?\"})\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a9f795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F927C85AB0>, search_kwargs={})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "  model = \"chatfire/bge-m3:q8_0\"\n",
    ")\n",
    "\n",
    "# ollama_embeddings를 활용하여 FAISS 벡터 저장소 생성 및 검색기 변환 \n",
    "# FAISS 벡터 저장소에 당신의 texts가 벡터화된 형태로 저장 \n",
    "retriever = FAISS.from_documents(texts, embeddings).as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b5a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 마이닝\n",
      "Embedding\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 2:\n",
      "\n",
      "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방\n",
      "문서 1:\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "Page Rank\n",
      "----------------------------------------------------------------------------------------------------\n",
      "문서 3:\n",
      "\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시\n"
     ]
    }
   ],
   "source": [
    "# 벡터 공간에서 유사도를 측정하여 상위 N개의 가장 유사한 문서에 대한 원본 텍스트 내용 반환\n",
    "docs = retriever.invoke(\"Embedding에 대해서 알려줘\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea0ddba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제공된 문서에는 임베딩에 대한 정의가 없습니다. 문서에는 페이지 랭크 알고리즘에 대한 설명, 예시, 그리고 관련 키워드만 포함되어 있습니다.\n",
      "\n",
      "따라서 질문에 대한 답변을 제공할 수 없습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## LLMChainExtractor를 활용하여 생성한 DocumentCompressor를 retriever에 적용한것이 \n",
    "## ContextualCompressionRetriever임.\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import LLMChain # LLMChainExtractor의 내부 상태 변경되어 LLMChain으로 넘겨줘야됨\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/test.txt\") # This is where test.txt content is loaded\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "ollama_embeddings = OllamaEmbeddings(\n",
    "  model = \"chatfire/bge-m3:q8_0\"\n",
    ")\n",
    "\n",
    "retriever = FAISS.from_documents(texts, ollama_embeddings).as_retriever()\n",
    "\n",
    "# chat 모델 생성 \n",
    "llm = ChatOllama(\n",
    "  model = \"gemma3:latest\",\n",
    "  base_url = \"http://localhost:11434\",\n",
    "  temperature=0.7,\n",
    ")\n",
    "\"\"\"\n",
    "OpenAI 활용 시, load_dotenv로 api_key 설정 \n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0,\n",
    "    model = \"gpt-4o-mini',\n",
    "    api_key = api_key\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"주어진 문서에서 질문과 관련된 핵심 정보를 추출하여 제공하세요. Let's think step by step\"),\n",
    "    (\"human\",\"문서: {context}\\n\\n질문:{question}\")\n",
    "])\n",
    "\n",
    "# LLMChain 생성\n",
    "llm_chain_for_extractor = LLMChain(llm = llm, prompt = prompt_template)\n",
    "\n",
    "# LLM을 기반으로 문서 내용 압축; 주어진 질의에 가장 관련성이 높은 문장이나 섹션만 추출 \n",
    "compressor = LLMChainExtractor(llm_chain=llm_chain_for_extractor)\n",
    "# base_retriever: 원본 문서를 가져올 기본 검색기\n",
    "# base_compressor: 가져온 문서를 압축할 컴프레서\n",
    "# 먼저 질문과 유사한 초기 관련 문서들을 다수 검색하여 유사도를 비교하고 컴프레서를 토해 필요한 정보만을 추출 및 요약함 \n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor,\n",
    "    base_retriever = retriever, #Embeddings 활용 \n",
    ")\n",
    "\n",
    "# question으로 사용 \n",
    "query = \"임베딩은 무엇인가요?\"\n",
    "# query가 포함된 핵심 정보만 추출되어 압축된 문서들의 목록을 반환 \n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e4a4c",
   "metadata": {},
   "source": [
    "## LLMChainFilter(LLM 기반 문서 필터링)\n",
    "- 초기에 검색된 문서 중 어떤 문서를 필터링하고 어떤 문서를 반활할지 결정하기 위해 LLM체인을 사용하는 보다 단순하지만 강력한 압축기 \n",
    "- `ContextualCompressionRetriever`에서 `LLMChainExtractor`은 문서에서 관련 부분만 추출\n",
    "- `LLMChainFilter`는 LLM을 사용하여 각 문서가 주어진 질문에 관련성이 있는지 없는지 판단하여 관련 없는 문서를 필터링하여 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa1ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "# 검색된 각 문서 전체가 질문에 관련성이 있는지 없는지를 판단하여 관련 없는 문서를 통째로 제거 \n",
    "compressor = LLMChainFilter(llm_chain = llm_chain_for_extractor)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = compressor,\n",
    "    base_retriever = retriever,\n",
    ")\n",
    "\n",
    "query = \"바다코끼리는 무엇인가요?\"\n",
    "compressed_docs = compression_retriever.get_relevant_documents(query)\n",
    "print(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed61a0",
   "metadata": {},
   "source": [
    "## EmbeddingsFilter\n",
    "\n",
    "- 각각의 검색된 문서에 대해 추가적인 LLM 호출을 수행하는 것은 비용이 많이 들고 속도가 느림\n",
    "- `EmbeddingsFilter`는 문서와 쿼리를 임베딩하고 쿼리와 충분히 유사한 임베딩을 가진 문서만 반환함으로써 더 저렴하고 빠름\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f078093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model = \"chatfire/bge-m3:q8_0\"\n",
    ")\n",
    "\n",
    "# 유사도 임곗값이 0.86인 EmbeddingsFilter 객체 생성 \n",
    "embeddings_filter = EmbeddingsFilter(\n",
    "    embeddings = embeddings,\n",
    "    similarity_threshold = 0.86\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor = embeddings_filter,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "compression_docs = compression_retriever.invoke(\n",
    "    \"Semantic Search에 대해서 알려줘\"\n",
    ")\n",
    "\n",
    "compression_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942f932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
